<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.5"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Antares Simulator: How does the automatic testing script system work ?</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Antares Simulator
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.5 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('md_src_tests_run_study_tests_readme.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">How does the automatic testing script system work ? </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="autotoc_md1"></a>
Introduction</h1>
<p >Here is an automatic testing python script system.</p>
<p >This program performs the following :</p><ol type="1">
<li>Searches for all studies in a given directory</li>
<li>From each study, retrieves the specifications to make checks on the simulation results of that study (see items below)</li>
<li>Runs a simlulation on each study</li>
<li>Given the results of the simulation on a study, makes the checks retrieved at step 2 on these results (for instance : make sure current results and reference resuts are identical, check for existence or content of output files,...)</li>
</ol>
<p >Note that each study found is supposed to contain the definition of checks performed by scripts after the simulation on the study is completed. So, each study is supposed to contain a file <b>check-config.json</b> for that purpose. This file is build manually for each study.</p>
<h1><a class="anchor" id="autotoc_md2"></a>
Entry points to the automatic testing script system</h1>
<p >For now, there are 2 entry points that can be used to run tests automatically.</p><ul>
<li>test_from_json.py</li>
<li>test_unfeasible_problem.py</li>
</ul>
<p >The first one runs tests as we explained above, the second one is a bit different : it does not search for studies, but is given an explicit path to a study. And this study does not contain any check definition under the form of a json file : checks are explicitly defined and supplied in the tests.</p>
<p >We won't comment the second script, as understanding the first one should ease understanding the second.</p>
<p ><span style="color:blue">TO DO : maybe we should think of integrating tests on unfeasible problems in the regular testing system ?</span></p>
<h1><a class="anchor" id="autotoc_md3"></a>
File test_from_json.py</h1>
<p >Here we examine the second part of script <b>test_from_json.py</b>. Note that The first part is dedicated to finding studies and storing the definition of the checks to be made on results after a simulation is run on studies (recall the <b>check-config.json</b> file). Let's look at the <b>test_from_json.py</b> (second part). </p><div class="fragment"><div class="line">1 <span class="preprocessor">@pytest.mark.json</span></div>
<div class="line">2 <span class="preprocessor">@pytest.mark.parametrize(&#39;study_path, test_check_data&#39;, json_collector.pairs()</span>, ids=json_collector.testIds())</div>
<div class="line">3 <span class="keyword">def </span>test(study_path, test_check_data, check_runner):</div>
<div class="line">4    checks = <a class="code hl_namespace" href="namespacecreate__checks.html">create_checks</a>(study_path, test_check_data, simulation=check_runner.get_simulation())</div>
<div class="line">5    check_runner.run(checks)</div>
<div class="ttc" id="anamespacecreate__checks_html"><div class="ttname"><a href="namespacecreate__checks.html">create_checks</a></div><div class="ttdef"><b>Definition:</b> create_checks.py:1</div></div>
</div><!-- fragment --><p> The content of this script is quite short, despite all the tests it has to perform. This is partially due to the power of <b>pytest</b> and its features : <b>fixtures</b> and especially <b>parametrization</b>, which is responsible for running a test many times, each time with a different set of arguments.</p>
<p >In the following, we comment the content of this script. Lines of this scripts are numbered so that they can be refered to whenever we need.</p>
<p ><b>Line 1</b> : Following tests are marked as a collection, that is belonging to the same category</p>
<h1><a class="anchor" id="autotoc_md4"></a>
Fixtures</h1>
<p ><b>pytest</b> comes with the notion of <b>fixture</b>. Fixtures allow executing a piece of code just before a test runs. To take bebefit of a fixture, a test needs to be given this fixture as argument. <a class="el" href="structFixture.html">Fixture</a> themselves can also be given arguments, we'll see how we do it (in the context of the current testing system) when we talk about <b>parametrization</b>. Fixtures return a result to be used in the test. Let's look at a simple test : </p><div class="fragment"><div class="line"><span class="preprocessor">@pytest.fixture</span></div>
<div class="line"><span class="keyword">def </span>my_fixture():</div>
<div class="line">    <span class="comment"># Define the result some_result to be returned (can be a variable or constant of any kind)</span></div>
<div class="line">    <span class="keywordflow">return</span> some_result</div>
<div class="line"> </div>
<div class="line"><span class="keyword">def </span>my_test(my_fixture):</div>
<div class="line">    <span class="comment"># Some use of &quot;my_fixture&quot;, as the result of the fixture &quot;my_fixture&quot; itself.</span></div>
<div class="line">    <span class="comment"># Some checks.</span></div>
</div><!-- fragment --><p> When <b>my_test</b> executes, it first calls <b>my_fixture</b>, which returns a result. This result can be anything. So it can be a constant (like a string) or an object.</p>
<p >Note that, when a test takes a fixture as argument, this argument is both a way to call the fixture and represents the result of the fixture itself. <br  />
 As a result, the argument can be (and should be) used in the test. We'll see examples of this in the following.</p>
<p >A fixture can be a bit more complex than the previously displayed one : it can be divided in 2 parts. The first would be a <b>setup</b> operation (executed just before the test begins) and the second part would be a <b>teardown</b> operation (executed just before the test ends). In the previous example, the fixture only has a <b>setup</b> part.</p>
<p >In order to supply a fixture with both <b>setup</b> and <b>teardown</b>, we need to use the <b>yield</b> python keyword. The <b>yield</b> instruction returns the fixture's result back to the test.</p>
<div class="fragment"><div class="line"><span class="preprocessor">@pytest.fixture</span></div>
<div class="line"><span class="keyword">def </span>my_fixture():</div>
<div class="line">    obj  = some_class(<span class="stringliteral">&quot;&quot;&quot;some args&quot;&quot;&quot;</span>)</div>
<div class="line">    <span class="keywordflow">yield</span> obj <span class="comment"># Returns obj before the test executes</span></div>
<div class="line">    <span class="comment"># Here some teardown instruction, therefore performed just before the test&#39;s execution ends.</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">def </span>my_test(my_fixture):</div>
<div class="line">    <span class="comment"># Some use of &quot;my_fixture&quot;, as the result of the fixture &quot;my_fixture&quot; itself.</span></div>
<div class="line">    <span class="comment"># Some checks.</span></div>
</div><!-- fragment --><p >Fixtures can be supplied with parameters. This can be done by giving an argument to the fixture. In the following snippet, the fixture <b>study_path</b> only returns the parameter that it's given. </p><div class="fragment"><div class="line"><span class="preprocessor">@pytest.fixture()</span></div>
<div class="line"><span class="keyword">def </span>study_path(request):</div>
<div class="line">    <span class="keywordflow">return</span> request.param</div>
</div><!-- fragment --><p> We'll see at least another example of such feature later on.</p>
<p >Another trait of fixtures is that they can be nested : a fixture can call other fixtures. <b>fixture 1</b> can have a <b>fixture 2</b> as an argument. This means that, when <b>fixture 1</b> comes to execute, <b>fixture 2</b> is called before execiting <b>fixture 1</b>'s body, and so <b>fixture 1</b> has access to <b>fixture 2</b>'s results during its execution.</p>
<h1><a class="anchor" id="autotoc_md5"></a>
Tests parametrization</h1>
<p >Back to script <b>test_from_json.py</b>.</p>
<p >Let's recall <b>line 2</b> : </p><div class="fragment"><div class="line">2 <span class="preprocessor">@pytest.mark.parametrize(&#39;study_path, test_check_data&#39;, json_collector.pairs()</span>, ids=json_collector.testIds())</div>
</div><!-- fragment --><p> Here is the place where we allow calling the body of our test multiple times. By "multiple time" we mean that the test will be run with different arguments each time.</p>
<p >In our example, fixture <b>study_path</b>, that is waiting for an argument, will be passed to the test, supplied with a different argument each time.</p>
<p >Same thing for the <b>test_check_data</b> value : it is not a feature (more a simple variable), but it will be passed to the test with a different value each time the test runs.</p>
<p >How do we do that ?</p>
<p >The first argument of the <b>parametrize</b> decorator (<span style="color:red">'study_path, test_check_data'</span>) represents the test's arguments to be changing each time the test runs.</p>
<p >The second argument is a list (<b>json_collector.pairs()</b>). Each element in the list is a pair (tuples with 2 elements) :</p><ul>
<li>the first value of the pair is a path to a particular study</li>
<li>the second value contains the data about checks that will be preformed on that study's results (after a simulation has run on that study).</li>
</ul>
<p >So, a test is run for each element of this list. It receives the first value of the pair as first argument, and the second value of the pair as second argument. This means that, for each test, fixture <b>study_path</b> receives as argument a path to a study, and that the variable <b>test_check_data</b> is supplied with an object containing all necessary data to performed a check.</p>
<p >Note : be aware that 2 pairs (study path, checks to do) can have the same study path : several checks can be made on the same study results.</p>
<p ><span style="color:blue">TO CHECK IN CODE : for a given study, can we have many checks requiring as much as simulation runs ? My guess is yes, but to be checked</span></p>
<h1><a class="anchor" id="autotoc_md6"></a>
The test's body</h1>
<p >With the previous explanations in mind, we're can describe what's happening when tests are run.</p>
<p >So the test is run multiple times due to <b>parametrization</b>. In fact, the test body is executed as many times as there are elements is the list <b>json_collector.pairs()</b>, that is as many studies spotted by the script <b>test_from_json.py</b> (see first part of the script, not commented in this doc). </p>
<h2><a class="anchor" id="autotoc_md7"></a>
&lt;ins&gt;Run fixtures&lt;/ins&gt;</h2>
<p >Here we talk about <b>line 3</b> of script <b>test_from_json.py</b>. </p><div class="fragment"><div class="line"><span class="keyword">def </span>test(study_path, test_check_data, check_runner):</div>
<div class="line">    ...</div>
</div><!-- fragment --><p> For a given run of the test's body, that is for each study previously found in a directory, some fixtures are first run.</p><ol type="1">
<li><b>study_path</b> : as already said above, this fixture is supplied with the path of the current study as only argument. Note that this fixture just returns this path, so this path is available for any use in the current test.</li>
<li><b>test_check_data</b> : is not a fixture, more a variable containing a definition of the check to be made on the current study's results after a simulation is run on the study.</li>
<li><b>check_runner</b> : this fixture does not seem to be the result of a parametrization, unlike the 2 first ones. So it seems it is called with the same argument for each study. It is not exactly the case because it calls other fixtures, that eventually call the <b>study_path</b> fixture (which is parametrized as we saw).</li>
</ol>
<p >Let's look at its content : </p><div class="fragment"><div class="line"><span class="preprocessor">@pytest.fixture(autouse=True)</span></div>
<div class="line"><span class="keyword">def </span>check_runner(simulation, resultsRemover):</div>
<div class="line">    <span class="comment"># Actions done before the current test</span></div>
<div class="line">    my_check_handler = check_handler(simulation, resultsRemover)</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># Running the current test here</span></div>
<div class="line">    <span class="keywordflow">yield</span> my_check_handler</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># Teardown : actions done after the current test</span></div>
<div class="line">    my_check_handler.teardown()</div>
</div><!-- fragment --><p> As we can see, this fixture is a <b>setup/teardown</b> fixture, and it calls 2 other fixtures (<b>simulation</b> and <b>resultsRemover</b>) before running. So when <b>check_runner</b> runs :</p><ul>
<li>A simulation is prepared (but not run yet)</li>
<li>A remove of resuls associated to this simulation is prepared (but not run yet)</li>
<li>A <b>check handler</b> object is returned to the current test, which will be able to use it under the form of its <b>check_runner</b> argument. We'll see that this <b>check handler</b> is responsible for running the simulation and for cleaning the simulation results after a test is completed.</li>
</ul>
<p >Note that the last line here will be executed when the current test ends.</p>
<h2><a class="anchor" id="autotoc_md8"></a>
&lt;ins&gt;Checks creation factory&lt;/ins&gt;</h2>
<p >This concerns <b>line 4</b> of script <b>test_from_json.py</b>. </p><div class="fragment"><div class="line">checks = <a class="code hl_namespace" href="namespacecreate__checks.html">create_checks</a>(study_path, test_check_data, simulation=check_runner.get_simulation())</div>
</div><!-- fragment --><p> Here we create a list of checks to perform on the results of a simulation on the current study is created.</p>
<p >This list is build from the description of the checks contained in <b>test_check_data</b>.</p>
<p >For that, <b>create_checks</b> is called, with natural information for a check : the path to the study used to fetch things to check, and te kind of thing to check (in <b>test_check_data</b>). The left argument of <b>create_checks</b> is the simulation previoulsy prepared by fixture <b>check_runner</b>. This simulation is needed for special kinds of check : the ones that need the return code of Antares Simulator just after it has run.</p>
<p >Note that all checks in that list is an instance of a class necessarily derived from the more general class <b>check_interface</b>. This parent class forced every child to have a method <b>run()</b> and a method <b>name()</b>, so that, when this list is traversed for any reason, a call to the <b>run()</b> or <b>name()</b> method does not fail.</p>
<p >Note also that, at this stage, the simulation has not been run yet, but will be run at the next line of <b>test_from_json.py</b>.</p>
<h2><a class="anchor" id="autotoc_md9"></a>
&lt;ins&gt;Running the checks&lt;/ins&gt;</h2>
<p >This is about <b>line 5</b> of script <b>test_from_json.py</b>. </p><div class="fragment"><div class="line">check_runner.run(checks)</div>
</div><!-- fragment --><p> This is the place where the script runs:</p><ol type="1">
<li>the simulation on the current study (this simulation was prepared when running fixture <b>check_runner</b> at test execution start)</li>
<li>the check(s) to be made on the results of this simulation (also prepared when fixture <b>check_runner</b> runs)</li>
</ol>
<p >Indeed the <b>run()</b> method of the <b>check_runner</b> fixture contains these 2 instructions.</p>
<h2><a class="anchor" id="autotoc_md10"></a>
&lt;ins&gt;End of tests&lt;/ins&gt;</h2>
<p >As we already saw, the <b>check_runner</b> fixture is a <b>setup/teardown</b> fixture. This means that when each test associated to a study ends, the <b>teardown</b> part of this fixture is run.</p>
<p >Looking at the content of <b>check_handler.teardown()</b> method, we see what it does :</p><ul>
<li>It puts back the study's state where it first was. Indeed, some checks modify the studies for their own purpose. The simulation is run with these new study parameters and checks are performed taking into account these changes. If several checks are made on the same study's results, and if every check requires a new simulation run, a tests can fail because a previous one made a change on the study that the current check does not expect. To avoid that, these changes are removed.</li>
<li>It removes the simulation results. Again, if several checks are made on a study, that may imply as much associated simulation runs, and in these conditions, retrieving some results' data in a study that has several simulation results directories can lead to an unexpected failure in tests.</li>
</ul>
<h1><a class="anchor" id="autotoc_md11"></a>
How to add a check on a test ?</h1>
<h2><a class="anchor" id="autotoc_md12"></a>
What to do ?</h2>
<ol type="1">
<li>Create a new python file in the <b>src/tests/run-study-tests/check_on_results</b></li>
<li>In this file, add the definition of a new class associated to the new check on a simulation run. This class must be derived from the parent class <b>check_interface</b>. This parent class forces you to define a <b>run()</b> and a <b>name()</b> method to your check class.</li>
<li>You'll probably want to add your new check to one or more studies. In this case, you need to add your check to the associated <b>check-config.json</b>. This implies :<ul>
<li>Defining a <b>json</b> representation of your check and how to insert it in a <b>check-config.json</b></li>
<li>Adding these definitions in a <b>json schema</b>. Note that every found <b>check-config.json</b> is verified against the <b>json schema</b> The schema can be found at : <b>src/tests/run-study-tests/parse_studies/json_schema.json</b></li>
</ul>
</li>
<li>Add the new check in the <b>check-config.json</b> files you need to.</li>
</ol>
<h2><a class="anchor" id="autotoc_md13"></a>
What NOT to do ?</h2>
<ul>
<li>When defining the class for your check, do not supply the constructor with treatments that require a simulation run : in the testing script system, the check instances are created before the simulation is run.</li>
<li>In the <b>json schema</b>, defining your check's grammar in the **"items" &gt; "properties" &gt; "checks" &gt; "properties"** part of is not enough : you must as well declare it in the list of possible checks in the **"items" &gt; "properties" &gt; "checks"**</li>
</ul>
<h1><a class="anchor" id="autotoc_md14"></a>
Running the script</h1>
<div class="fragment"><div class="line">&gt; cd src/tests/run-study-tests</div>
<div class="line">&gt; python -m pytest -m mps --solver-path=/path/to/the/Antares/solver/antares-x.y-solver.exe</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md15"></a>
TO DO</h1>
<p >Doc : Clarify if a check in the <b>check-config.json</b> can be composed of several sub-checks, and how it works in this case. Code : </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.5 </li>
  </ul>
</div>
</body>
</html>
